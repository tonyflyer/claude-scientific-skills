# Common Issues in Scientific Papers

A comprehensive guide to the most frequent problems found during paper validation, organized by review area.

---

## Arguments Issues

### 1. Unsupported Claims

**Problem:** Statements presented as facts without evidence

**Examples:**
```
‚ùå "Our method is significantly better than all existing approaches."
   (No evidence, no comparison, no metrics)

‚ùå "Deep learning has solved the problem of image recognition."
   (Overgeneralization, ignores limitations)

‚ùå "This proves that our hypothesis is correct."
   (Confusion between correlation and causation)
```

**Fix:**
```
‚úÖ "Our method achieves 95.3% accuracy on ImageNet, outperforming
   the previous best result of 92.1% [Citation] by 3.2 percentage points."

‚úÖ "Deep learning methods have achieved human-level performance on
   specific image recognition benchmarks [Citations], though challenges
   remain in adversarial robustness and few-shot learning."

‚úÖ "These results support our hypothesis that [X], though alternative
   explanations such as [Y] cannot be ruled out."
```

**Validator Question:**
"Can you provide experimental evidence supporting the claim in Line 145 that [your method] is better than existing approaches? Including specific metrics and comparisons would strengthen this claim."

---

### 2. Logical Fallacies

**Common fallacies:**

#### Post Hoc Ergo Propter Hoc
```
‚ùå "Sales increased after we launched the campaign, therefore the
   campaign caused the increase."
```
**Issue:** Correlation doesn't imply causation

#### Hasty Generalization
```
‚ùå "We tested on three samples and observed X, therefore X is always true."
```
**Issue:** Insufficient sample size

#### Circular Reasoning
```
‚ùå "Method A is better because it achieves higher performance, and it
   achieves higher performance because it's a better method."
```
**Issue:** Conclusion assumed in premise

#### False Dichotomy
```
‚ùå "Either we use our method or we accept poor performance."
```
**Issue:** Ignores other alternatives

**Validator Question:**
"The reasoning in Section 3.2 appears to assume [X] when concluding [Y]. Could you clarify the logical connection or provide additional steps in the argument?"

---

### 3. Missing Logical Steps

**Problem:** Jumping from evidence to conclusion without intermediate steps

**Example:**
```
‚ùå "We observed that parameter Œ± affects performance. Therefore,
   our method is superior to baselines."
```

**Missing steps:**
1. How does Œ± relate to baseline methods?
2. What parameter values did baselines use?
3. Is the comparison fair?

**Fix:**
```
‚úÖ "We observed that parameter Œ± significantly affects performance
   (Table 2). Unlike baseline methods [Citations] which fix Œ±=0.5,
   our method learns Œ± adaptively. When baselines are enhanced with
   learned Œ±, performance improves but remains below our method
   (Table 3), suggesting our gains come from both adaptive Œ± and
   our novel architecture."
```

---

## Clarity Issues

### 1. Ambiguous Pronouns

**Problem:** "It", "this", "that" without clear antecedent

**Examples:**
```
‚ùå "The model processes input and generates output. It then optimizes
   the result."
   (What does "it" refer to? Model? Input? Output?)

‚ùå "We trained on dataset A and tested on dataset B. This showed
   better performance."
   (What showed better performance? Training? Testing? The dataset?)
```

**Fix:**
```
‚úÖ "The model processes input and generates output. The model then
   optimizes the generated output."

‚úÖ "We trained on dataset A and tested on dataset B. Testing on B
   showed better generalization than in-domain evaluation."
```

**Validator Question:**
"In Section 3.2, Line 156, what does 'it' refer to in the sentence [quote]? Clarifying this would improve readability."

---

### 2. Undefined Technical Terms

**Problem:** Using jargon or specialized terms without definition

**Examples:**
```
‚ùå "We use a GAN to generate samples."
   (First use of "GAN" - undefined)

‚ùå "The model exhibits strong OOD performance."
   (What is "OOD"?)

‚ùå "We employ a transformer-based architecture."
   (For general audience, needs brief explanation)
```

**Fix:**
```
‚úÖ "We use a Generative Adversarial Network (GAN) [Citation] to
   generate samples. GANs consist of..."

‚úÖ "The model exhibits strong out-of-distribution (OOD) performance,
   meaning it generalizes well to data from different domains."

‚úÖ "We employ a transformer-based architecture [Citation], which uses
   self-attention mechanisms to process sequential data."
```

---

### 3. Inconsistent Terminology

**Problem:** Using multiple terms for the same concept

**Examples:**
```
‚ùå Using "model", "system", "method", "approach", "technique"
   interchangeably for the same thing

‚ùå Switching between "accuracy", "performance", "effectiveness"

‚ùå "dataset" vs. "data set" vs. "data-set" inconsistently
```

**Fix:**
```
‚úÖ Choose one term (e.g., "method") and use consistently
‚úÖ Use "accuracy" for metrics, "performance" for overall evaluation
‚úÖ Standardize spelling: "dataset" (one word)
```

**Validator Question:**
"The paper uses 'model', 'system', and 'method' interchangeably. Consider standardizing to a single term for consistency."

---

### 4. Vague Quantifiers

**Problem:** Using imprecise terms without specifics

**Examples:**
```
‚ùå "We tested on a large dataset."
   (How large?)

‚ùå "Many papers have addressed this problem."
   (How many? Which papers?)

‚ùå "Our method is significantly faster."
   (How much faster? Statistical significance?)

‚ùå "The model performs well."
   (What metric? What value?)
```

**Fix:**
```
‚úÖ "We tested on ImageNet, which contains 1.2 million training images."

‚úÖ "15 papers published at top conferences (ICML, NeurIPS, ICLR) in
   the last 3 years have addressed this problem [Citations]."

‚úÖ "Our method is 3.2√ó faster than the best baseline (p < 0.01),
   reducing training time from 24 hours to 7.5 hours."

‚úÖ "The model achieves 94.7% accuracy on the test set, exceeding
   the 90% threshold required for deployment."
```

---

## Evidence Issues

### 1. Missing Baselines

**Problem:** Comparing only against weak or outdated methods

**Examples:**
```
‚ùå Comparing against methods from >5 years ago
‚ùå Comparing only against your own prior work
‚ùå Omitting the current state-of-the-art
‚ùå Cherry-picking easy baselines
```

**Fix:**
```
‚úÖ Include recent SOTA methods from last 2 years
‚úÖ Compare with multiple strong baselines
‚úÖ Justify any omissions ("Method X is not comparable because...")
‚úÖ Use standard benchmarks where available
```

**Validator Question:**
"Method Z [Citation, Year] appears to be the current state-of-the-art for this task according to [Benchmark]. How does your method compare? Including this comparison would better position your contribution."

---

### 2. No Statistical Significance

**Problem:** Reporting results without error bars or significance tests

**Examples:**
```
‚ùå Table showing: "Our method: 92.3%, Baseline: 91.9%"
   (Is 0.4% difference significant?)

‚ùå Claiming "improved performance" with only single runs
‚ùå No confidence intervals reported
‚ùå No discussion of variance across runs
```

**Fix:**
```
‚úÖ "Our method: 92.3 ¬± 0.4%, Baseline: 91.9 ¬± 0.5% (p < 0.01, t-test)"
‚úÖ Report mean ¬± std over multiple runs (typically 3-5)
‚úÖ Include confidence intervals when appropriate
‚úÖ Discuss variance: "Results are consistent across runs (low variance)"
```

**Validator Question:**
"Can you provide statistical significance tests for the results in Table 2? The improvements are small (0.4-0.8%), so demonstrating statistical significance would strengthen the claims."

---

### 3. Missing Ablations

**Problem:** Not showing which components contribute to performance

**Examples:**
```
‚ùå Proposing method with 3 novel components A, B, C but only
   evaluating full system

‚ùå No analysis of which component matters most
‚ùå No study of component interactions
‚ùå Can't determine if all components are necessary
```

**Fix:**
```
‚úÖ Ablation table showing:
   - Full system: 94.5%
   - Without A: 92.1% (A contributes 2.4%)
   - Without B: 93.8% (B contributes 0.7%)
   - Without C: 90.3% (C contributes 4.2%)
   - Baseline: 89.5%

‚úÖ Conclusion: "Component C contributes most (4.2%), followed by A
   (2.4%). Component B has smaller but consistent impact (0.7%)."
```

**Validator Question:**
"Your method has three key components (A, B, C mentioned in Section 3). Can you provide an ablation study showing the contribution of each? This would justify the design choices and show which components are most important."

---

### 4. Limited Evaluation Scope

**Problem:** Testing only on one dataset or scenario

**Examples:**
```
‚ùå Evaluating only on MNIST (too easy)
‚ùå Testing only one domain
‚ùå Single dataset evaluation
‚ùå No cross-dataset generalization
‚ùå No robustness tests
```

**Fix:**
```
‚úÖ Test on multiple datasets from different domains
‚úÖ Include cross-dataset evaluation (train on A, test on B)
‚úÖ Test robustness to noise, corruptions, adversarial examples
‚úÖ Evaluate on different data scales
‚úÖ Test failure cases and limitations
```

**Validator Question:**
"The evaluation is limited to [Dataset X]. Would testing on additional datasets such as [Y] or [Z] help demonstrate the generalizability of your approach?"

---

## Alternatives Issues

### 1. Unexamined Assumptions

**Problem:** Making assumptions without justification or testing

**Examples:**
```
‚ùå "We assume the data is i.i.d."
   (Is this tested? What if violated?)

‚ùå "We assume labels are noise-free."
   (Realistic? What about noisy labels?)

‚ùå "We assume computational resources are unlimited."
   (What about resource-constrained settings?)
```

**Fix:**
```
‚úÖ "We assume data is i.i.d., which holds for standard benchmark
   datasets [Citation]. Section 5.3 examines performance when this
   assumption is violated (non-i.i.d. scenarios)."

‚úÖ "While we assume noise-free labels for main experiments, Section
   5.4 evaluates robustness to label noise (5-20% corruption rates)."

‚úÖ "Main experiments assume typical research settings (GPU access).
   Section 5.5 evaluates resource-constrained scenarios."
```

**Validator Question:**
"The method assumes [X] (stated in Section 3.1). How would performance degrade if this assumption doesn't hold? A brief robustness analysis would strengthen the paper."

---

### 2. Ignoring Failure Cases

**Problem:** Only showing successes, not discussing when method fails

**Examples:**
```
‚ùå No discussion of limitations
‚ùå Only showing cherry-picked examples
‚ùå Not examining failure modes
‚ùå No error analysis
```

**Fix:**
```
‚úÖ "Our method performs well on [scenarios], but struggles with
   [specific cases]. Figure 5 shows failure examples where..."

‚úÖ "Limitations: (1) Requires large training data (10K+ samples),
   (2) Struggles with extreme class imbalance (>100:1),
   (3) Computational cost limits real-time applications."

‚úÖ "Error analysis (Section 5.6) reveals most failures occur in
   [specific scenario]. This suggests future work should focus on..."
```

---

### 3. Missing Alternative Explanations

**Problem:** Not considering other interpretations of results

**Examples:**
```
‚ùå "Method A beats Method B, therefore A's architecture is better."
   (Could be: hyperparameters, implementation, dataset bias, luck)

‚ùå "Adding component X improves performance, so X is necessary."
   (Could be: any regularization helps, overfitting reduced, etc.)
```

**Fix:**
```
‚úÖ "Method A outperforms B in our experiments. To isolate the
   architectural contribution, we: (1) tuned both equally carefully,
   (2) used the same implementation framework, (3) averaged over 5
   seeds. This suggests architectural differences drive the gains,
   though further investigation of training dynamics is warranted."

‚úÖ "Adding component X improves performance. We hypothesize this is
   due to [specific mechanism]. To test this, we compare with
   alternative regularization approaches (Table 4), which show
   smaller gains, supporting our hypothesis."
```

---

## Novelty Issues

### 1. Unclear Contribution

**Problem:** Not clearly stating what's new

**Examples:**
```
‚ùå "We propose a novel deep learning method."
   (What's novel? Architecture? Loss? Training?)

‚ùå Introduction doesn't highlight contributions
‚ùå Related work doesn't differentiate from prior work
‚ùå Methods section looks like incremental change
```

**Fix:**
```
‚úÖ "Our contributions are: (1) A new attention mechanism that...
   Unlike prior work [Citation] which..., our approach...,
   (2) A training procedure that..., and (3) State-of-the-art
   results on 3 benchmarks, with 4.2% average improvement."

‚úÖ "While [Prior Work] proposed [X], they assumed [limitation].
   Our key insight is [new idea], which enables [capability]."
```

---

### 2. Missing Related Work Citations

**Problem:** Not citing relevant prior work

**Examples:**
```
‚ùå Claiming first to do X when similar work exists
‚ùå Ignoring concurrent work
‚ùå Missing key papers in the field
‚ùå Outdated related work (all citations >3 years old)
```

**Fix:**
```
‚úÖ Comprehensive literature review
‚úÖ Include work from last 2 years
‚úÖ Cite both historical foundations and recent advances
‚úÖ Acknowledge concurrent work if applicable
‚úÖ Differentiate clearly from each cited work
```

**Validator Question:**
"Reference [X, Year] appears to propose a similar approach using [technique]. Can you clarify how your work differs in Section 2? Explicitly stating the key differences would strengthen the novelty claim."

---

### 3. Overclaimed Novelty

**Problem:** Claiming too much novelty for incremental work

**Examples:**
```
‚ùå "We propose the first method to..."
   (When prior work exists)

‚ùå "Revolutionary approach that..."
   (Hyperbole without evidence)

‚ùå Calling standard technique "novel"
   (E.g., calling batch normalization novel in 2024)
```

**Fix:**
```
‚úÖ "We extend [Prior Work] by adding [specific modification],
   which enables [new capability]."

‚úÖ "Our contributions are incremental but important: we show that
   [small change] leads to [significant impact]."

‚úÖ "While [technique] is well-established, our application to
   [new domain] is, to our knowledge, the first demonstration of..."
```

---

## Confusion Issues

### 1. Poor Organization

**Problem:** Illogical structure or missing sections

**Examples:**
```
‚ùå Methods described before motivation
‚ùå Results presented before methods
‚ùå Related work at the end (hard to position contributions)
‚ùå Jumping between topics within sections
‚ùå No clear narrative flow
```

**Fix:**
```
‚úÖ Standard structure:
   Abstract ‚Üí Introduction ‚Üí Related Work (or after Methods) ‚Üí
   Methods ‚Üí Results ‚Üí Discussion ‚Üí Conclusion

‚úÖ Each section has clear purpose
‚úÖ Smooth transitions between sections
‚úÖ Logical progression of ideas
```

---

### 2. Notation Before Definition

**Problem:** Using symbols before defining them

**Examples:**
```
‚ùå Equation 3 uses Œ∏, defined later in Section 3.3
‚ùå Figure 2 shows Œ±, Œ≤ without explanation
‚ùå Algorithm uses notation not in text
```

**Fix:**
```
‚úÖ Define notation when first used
‚úÖ Include notation table if many symbols
‚úÖ Be consistent across equations, figures, algorithms
‚úÖ Use standard notation when possible
```

**Validator Question:**
"Notation Œ∏ is used in Equation 3 before being defined. Can you define it when first introduced? This would prevent reader confusion."

---

### 3. Missing Intuitive Explanation

**Problem:** Jumping straight into math without intuition

**Examples:**
```
‚ùå Section starts with complex equation
‚ùå No example before formalization
‚ùå No figure showing key idea
‚ùå Technical details before big picture
```

**Fix:**
```
‚úÖ Start with intuitive explanation
‚úÖ Provide simple example
‚úÖ Show figure illustrating concept
‚úÖ Then give formal definition
‚úÖ Walk through example using formal definition
```

**Example structure:**
```
"Intuitively, our method works by [simple explanation]. For example,
consider [concrete case]. Now we formalize this intuition. Let X be
[definition]... Returning to our example, we see that..."
```

---

## Severity Assessment

### How to Determine Severity

**üî¥ Major (Must Fix):**
- Affects validity of results
- Fundamental flaws in methodology
- Main claims unsupported
- Critical experiments missing
- Prevents publication

**üü° Moderate (Should Fix):**
- Affects interpretation of results
- Missing important comparisons
- Unclear explanations of key points
- Limits impact or generalizability
- Weakens but doesn't block publication

**üü¢ Minor (Nice to Fix):**
- Affects presentation quality
- Grammar, typos, formatting
- Minor clarity improvements
- Doesn't affect scientific validity
- Polish for better readability

---

## Prevention Strategies

### During Writing
1. **Define terms immediately** - Don't use before defining
2. **Support all claims** - Add [Citation needed] markers
3. **Be specific** - Avoid "many", "large", "significant" without numbers
4. **Test assumptions** - Include robustness experiments early
5. **Ablate components** - Test each independently

### Before Submission
1. **Fresh read-through** - Wait 24 hours, read again
2. **Read aloud** - Catches awkward phrasing
3. **Peer review** - Have colleague read
4. **Use paper-validator** - Systematic check
5. **Compare with accepted papers** - Match quality standards

---

This reference is part of the Claude Scientific Skills ecosystem.
Use the `paper-validator` skill for systematic issue detection.
